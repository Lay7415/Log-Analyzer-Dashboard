### docker-compose.yml

```
services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.4.0
    container_name: kafka
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
    ports:
      - "9092:9092"
    depends_on:
      - zookeeper

  nginx:
    image: nginx:latest
    container_name: nginx
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/site1:/usr/share/nginx/html
      - ./nginx/logs:/var/log/nginx
    ports:
      - "8080:80"

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.10.2
    container_name: filebeat
    user: root
    command: ["--strict.perms=false"]
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./nginx/logs:/var/log/nginx:ro
    depends_on:
      - kafka
      - log_generator

  clickhouse:
    image: yandex/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - ./clickhouse/init.sql:/docker-entrypoint-initdb.d/init.sql

  spark:
    build: ./spark
    container_name: spark
    user: root
    volumes:
      - ./spark:/opt/spark-apps  
      - ./spark/ivy:/home/spark/.ivy2
    depends_on:
      - kafka
      - clickhouse
    command: >
      bash -c "/opt/spark/bin/spark-submit
      --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,com.clickhouse:clickhouse-jdbc:0.4.6
      /opt/spark-apps/spark_processor.py"

  log_generator:
      image: python:3.11-slim
      container_name: log_generator
      volumes:
        - ./nginx/logs:/var/log/nginx
        - ./generate_logs.py:/app/generate_logs.py
      # –£–î–ê–õ–ò–¢–¨ command, —á—Ç–æ–±—ã –æ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª CMD/ENTRYPOINT –∏–∑ Dockerfile –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ —Ä–∞–±–æ—Ç–∞–ª
      # command: >
      #   sh -c "pip install faker && python3 /app/generate_logs.py && echo 'Log generation finished.'"
      command: sh -c "pip install faker && python3 /app/generate_logs.py" # –û—Å—Ç–∞–≤–∏—Ç—å, —á—Ç–æ–±—ã —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å faker –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫—Ä–∏–ø—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ
      depends_on:
        - nginx

  streamlit:
    build:
      context: ./streamlit
    container_name: streamlit
    ports:
      - "8501:8501"
    depends_on:
      - clickhouse
```

### generate_logs.py

```
import random
import time
from datetime import datetime, timezone, timedelta
from faker import Faker
import os

fake = Faker()

ACCESS_LOG_FILE_PATH = "/var/log/nginx/access.log" 
ERROR_LOG_FILE_PATH = "/var/log/nginx/error.log"
LOG_INTERVAL = 1 

LOG_DIR = os.path.dirname(ACCESS_LOG_FILE_PATH)
if not os.path.exists(LOG_DIR):
    os.makedirs(LOG_DIR, exist_ok=True)
    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {LOG_DIR} —Å–æ–∑–¥–∞–Ω–∞.")

print("–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—É–ª–æ–≤ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è access.log...")

IP_POOL = [fake.ipv4() for _ in range(200)]
ip_weights = ([0.04] * 10) + ([0.00315] * 190)

pages = [
    "/", "/products/123", "/api/v1/users", "/cart", "/login",
    "/products/456", "/checkout", "/blog/article-1", "/contact-us", "/api/v2/items",
    "/admin/panel", "/static/style.css", "/images/logo.png", "/uploads/document.pdf"
]
page_weights = [0.25, 0.15, 0.10, 0.10, 0.08, 0.08, 0.05, 0.04, 0.04, 0.03, 0.01, 0.01, 0.01, 0.01]

USER_AGENT_POOL = [fake.user_agent() for _ in range(100)]
http_statuses = [200, 301, 404, 500, 403]
status_weights = [0.8, 0.05, 0.08, 0.02, 0.05]

print("–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –¥–ª—è –ª–æ–≥–æ–≤...")

current_time = datetime.now(timezone.utc)
end_time = current_time
start_time = end_time - timedelta(hours=6)
peak_time_start = end_time - timedelta(hours=3)
peak_time_end = end_time - timedelta(hours=1)

payloads = {
    "1.2.3.4": ["/some/path", "/admin", "/login", "/api/v1/users"],
    "5.6.7.8": ["' OR '1'='1", "admin' --", "' UNION SELECT * FROM users --"],
    "9.10.11.12": ["<script>alert('xss')</script>", "<img src=x onerror=alert('xss')>"],
    "11.22.33.44": ["' OR 1=1--", " UNION SELECT user, password FROM users--", " 1' AND '1'='1"],
    "55.66.77.88": ["../../../../etc/passwd", "../../../../../windows/system.ini"],
    "99.88.77.66": ["/wp-admin/", "/phpmyadmin/", "/.git/config", "/solr/admin/"],
}

def generate_attack_log(ip, request_template, status_code, is_anomaly=1, anomaly_type=""):
    log_time = datetime.now(timezone.utc) - timedelta(seconds=random.randint(1, 10)) # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –Ω–µ–º–Ω–æ–≥–æ –≤ –ø—Ä–æ—à–ª–æ–º
    timestamp_str = log_time.strftime('%d/%b/%Y:%H:%M:%S %z')
    
    if ip in payloads and anomaly_type in ["SQL Injection", "Path Traversal", "Vulnerability Scanning"]:
        payload = random.choice(payloads.get(ip, [""]))
        page = request_template.format(payload=payload)
    else:
        page = request_template 
    
    method = random.choice(["GET", "POST"]) if not anomaly_type else ("GET" if "Scanning" in anomaly_type or "Traversal" in anomaly_type else "POST")
    request = f"{method} {page} HTTP/1.1"
    bytes_sent = random.randint(200, 15000)
    referrer = fake.uri()
    user_agent = random.choice(USER_AGENT_POOL)
    
    if anomaly_type == "Login Attack":
        request = "POST /login HTTP/1.1"
        status_code = 401
        bytes_sent = 500

    log_line = f'{ip} - - [{timestamp_str}] "{request}" {status_code} {bytes_sent} "{referrer}" "{user_agent}"\n'
    
    with open(ACCESS_LOG_FILE_PATH, "a") as f:
        f.write(log_line)

print(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ª–æ–≥–æ–≤ –≤ {ACCESS_LOG_FILE_PATH} —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {LOG_INTERVAL} —Å–µ–∫...")

try:
    with open(ACCESS_LOG_FILE_PATH, "w") as f:
        f.write("") 
    with open(ERROR_LOG_FILE_PATH, "w") as f:
        f.write("") 
except Exception as e:
    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ–∞–π–ª–æ–≤ –ª–æ–≥–æ–≤: {e}")


next_attack_time = time.time() + random.randint(5, 20)
next_login_time = time.time() + random.randint(10, 30)
next_scan_time = time.time() + random.randint(15, 40)
next_error_time = time.time() + random.uniform(0.5, 1.5)

while True:
    start_loop_time = time.time()
    current_ts = time.time()
    
    log_time = datetime.now(timezone.utc)
    timestamp_str = log_time.strftime('%d/%b/%Y:%H:%M:%S %z')
    ip = random.choices(IP_POOL, weights=ip_weights, k=1)[0]
    page = random.choices(pages, weights=page_weights, k=1)[0]
    user_agent = random.choice(USER_AGENT_POOL)
    method = random.choice(["GET", "POST"])
    request = f"{method} {page} HTTP/1.1"
    status = random.choices(http_statuses, weights=status_weights, k=1)[0]
    bytes_sent = random.randint(100, 15000)
    referrer = fake.uri()
    log_line = f'{ip} - - [{timestamp_str}] "{request}" {status} {bytes_sent} "{referrer}" "{user_agent}"\n'
    
    with open(ACCESS_LOG_FILE_PATH, "a") as f: 
        f.write(log_line)
        
    if current_ts > next_attack_time:
        print("Injecting: Request Rate Anomaly")
        for _ in range(random.randint(5, 10)): 
             generate_attack_log("1.2.3.4", "/some/path", 403, 1, "Request Rate Anomaly")
        next_attack_time = current_ts + random.randint(10, 30)
        
    if current_ts > next_login_time:
        print("Injecting: Login Attack")
        for _ in range(20):
             generate_attack_log("10.20.30.40", "/login", 401, 1, "Login Attack")
        next_login_time = current_ts + random.randint(30, 60)
        
    if current_ts > next_scan_time:
        print("Injecting: Scanning Activity")
        for page in {fake.uri_path() for _ in range(random.randint(10, 20))}:
            generate_attack_log(f"50.60.70.{random.randint(1, 254)}", f"{page}", 404, 1, "Scanning Activity")
        next_scan_time = current_ts + random.randint(40, 90)

    if random.random() < 0.05: 
        generate_attack_log("11.22.33.44", "/products?id={payload}", 500, 1, "SQL Injection")
        
    if random.random() < 0.03: 
        generate_attack_log("55.66.77.88", "/static/{payload}", 403, 1, "Path Traversal")

    if random.random() < 0.02: 
        generate_attack_log("99.88.77.66", "{payload}", 404, 1, "Vulnerability Scanning")
        
    if random.random() < 0.01: 
        user_agent = random.choice(["sqlmap", "Nikto", "Nmap Scripts"])
        log_time_ua = datetime.now(timezone.utc) - timedelta(seconds=random.randint(1, 5))
        timestamp_str_ua = log_time_ua.strftime('%d/%b/%Y:%H:%M:%S %z')
        with open(ACCESS_LOG_FILE_PATH, "a") as f:
            f.write(f'44.55.66.77 - - [{timestamp_str_ua}] "GET / HTTP/1.1" 200 1200 "{fake.uri()}" "{user_agent}"\n')


    if current_ts > next_error_time:
        log_time_err = datetime.now(timezone.utc) - timedelta(seconds=random.randint(1, 3))
        timestamp_str_err = log_time_err.strftime('%Y/%m/%d %H:%M:%S')
        level = random.choice(["error", "warn"])
        message = random.choice([
            'open() "/usr/share/nginx/html/favicon.ico" failed (2: No such file or directory)',
            'directory index of "/usr/share/nginx/html/images/" is forbidden',
            'access forbidden by rule',
            'client sent invalid method while reading client request line'
        ])
        ip = random.choice(IP_POOL)
        log_line = f'{timestamp_str_err} [{level}] 12345#12345: *6789 client: {ip}, server: localhost, request: "GET /some/problematic/path HTTP/1.1", {message}, host: "localhost:8080"\n'
        with open(ERROR_LOG_FILE_PATH, "a") as f:
            f.write(log_line)
        next_error_time = current_ts + random.uniform(0.5, 1.5)
        
    
    elapsed = time.time() - start_loop_time
    sleep_time = LOG_INTERVAL - elapsed
    if sleep_time > 0:
        time.sleep(sleep_time)
```

### filebeat.yml

```
filebeat.inputs:
- type: log
  enabled: true
  paths:
  - /var/log/nginx/access.log
  fields:
    log_type: "access"
  fields_under_root: true

- type: log
  enabled: true
  paths:
  - /var/log/nginx/error.log
  fields:
    log_type: "error"
  fields_under_root: true
  multiline:
    pattern: '^[0-9]{4}/[0-9]{2}/[0-9]{2}'
    negate: true
    match: after

output.kafka:
  hosts: [ "kafka:9092" ]
  topic: "nginx_logs"
  codec.json:
    pretty: false
  required_acks: 1
  max_message_bytes: 1000000

```

### .gitignore

```
./spark/GeoLite2-Country_20251028
./presentation.md
```

### nginx/nginx.conf

```
events {}

http {
    access_log /var/log/nginx/access.log;
    error_log /var/log/nginx/error.log;

    server {
        listen 80;
        server_name localhost;
        root /usr/share/nginx/html;
        index index.html;
    }
}

```

### nginx/site1/index.html

```
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <title>Test Site 1</title>
</head>
<body>
    <h1>–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å!</h1>
    <p>–≠—Ç–æ —Ç–µ—Å—Ç–æ–≤–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –¥–ª—è –ª–æ–≥–æ–≤ Nginx.</p>
    <a href="/about.html">–û —Å–∞–π—Ç–µ</a>
</body>
</html>

```

### clickhouse/init.sql

```
-- clickhouse/init.sql

-- 1. –¢–∞–±–ª–∏—Ü–∞ –ò–∑–º–µ—Ä–µ–Ω–∏—è: –í—Ä–µ–º—è (–¥–ª—è –ª—É—á—à–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏/–∞–Ω–∞–ª–∏–∑–∞, —Ö–æ—Ç—è –≤ CH DateTime –∏ —Ç–∞–∫ —Ö–æ—Ä–æ—à)
CREATE TABLE IF NOT EXISTS dim_time (
    time_id DateTime,
    hour UInt8,
    day_of_week UInt8,
    is_weekend UInt8
) ENGINE = MergeTree()
ORDER BY time_id;

-- 2. –¢–∞–±–ª–∏—Ü–∞ –ò–∑–º–µ—Ä–µ–Ω–∏—è: IP / –ì–µ–æ–ª–æ–∫–∞—Ü–∏—è
-- –ò—Å–ø–æ–ª—å–∑—É–µ–º MATERIALIZED/ReplacingMergeTree –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–ª—é—á–∞ (ip_id) –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è/—É–¥–∞–ª–µ–Ω–∏—è —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π
CREATE TABLE IF NOT EXISTS dim_ip (
    ip_id UInt64 MATERIALIZED toUInt64(abs(cityHash64(ip))), -- –ü—Ä–æ—Å—Ç–æ–π —Ö—ç—à –∫–∞–∫ ID
    ip String,
    country LowCardinality(String)
) ENGINE = ReplacingMergeTree(ip_id) -- –ò—Å–ø–æ–ª—å–∑—É–µ–º Replacing –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–ø–∏—Å–µ–π —Å —Ç–µ–º –∂–µ IP
ORDER BY ip;

-- 3. –¢–∞–±–ª–∏—Ü–∞ –ò–∑–º–µ—Ä–µ–Ω–∏—è: –¢–∏–ø—ã –ê–Ω–æ–º–∞–ª–∏–π –∏ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
CREATE TABLE IF NOT EXISTS dim_anomaly_type (
    anomaly_type_id UInt8 MATERIALIZED toUInt8(abs(cityHash64(anomaly_type, is_anomaly) % 255)), -- –ü—Ä–æ—Å—Ç–æ–π ID
    anomaly_type String,
    is_anomaly UInt8 -- 1 –∏–ª–∏ 0
) ENGINE = ReplacingMergeTree(anomaly_type_id)
ORDER BY anomaly_type;

-- 4. –¢–∞–±–ª–∏—Ü–∞ –§–∞–∫—Ç–æ–≤: –°–æ–±—ã—Ç–∏—è Nginx
CREATE TABLE IF NOT EXISTS fact_nginx_requests (
    -- –ö–ª—é—á–∏ –ò–∑–º–µ—Ä–µ–Ω–∏–π (Foreign Keys)
    time_key DateTime,        -- –ö–ª—é—á –≤—Ä–µ–º–µ–Ω–∏ (—Å—Å—ã–ª–∫–∞ –Ω–∞ dim_time, –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ DateTime)
    ip_key UInt64,             -- –ö–ª—é—á IP (—Å—Å—ã–ª–∫–∞ –Ω–∞ dim_ip.ip_id)
    anomaly_type_key UInt8,    -- –ö–ª—é—á —Ç–∏–ø–∞ –∞–Ω–æ–º–∞–ª–∏–∏ (—Å—Å—ã–ª–∫–∞ –Ω–∞ dim_anomaly_type.anomaly_type_id)
    log_type LowCardinality(String), -- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ access/error

    -- –ú–µ—Ç—Ä–∏–∫–∏ (Values)
    status UInt16,
    bytes UInt32,

    -- –¢–µ–∫—Å—Ç –æ—à–∏–±–∫–∏ (–¥–ª—è –æ—à–∏–±–æ–∫, –Ω–µ –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏)
    error_message Nullable(String),

    -- –ü–æ–ª—è –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞ (–¥–ª—è –ª–æ–≥–æ–≤ –æ—à–∏–±–æ–∫)
    method Nullable(String),
    page Nullable(String)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(time_key)
ORDER BY (time_key, ip_key);


-- –¢–∞–±–ª–∏—Ü–∞ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –æ—Å—Ç–∞–µ—Ç—Å—è
CREATE TABLE IF NOT EXISTS nginx_predictions (
    timestamp DateTime,
    predicted_requests Float64,
    predicted_lower Float64,
    predicted_upper Float64
) ENGINE = MergeTree()
ORDER BY timestamp;
```

### streamlit/dashboard.py

```
import streamlit as st
import pandas as pd
from clickhouse_driver import Client
import plotly.express as px
import pycountry_convert as pc
from datetime import datetime, timedelta
import altair as alt

# --- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ë–î ---
st.set_page_config(page_title="Log Dashboard", layout="wide")

@st.cache_resource
def get_clickhouse_client():
    client = Client(host="clickhouse", port=9000)
    return client

CLIENT = get_clickhouse_client()


# --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ---
@st.cache_data(ttl=60)
def run_query(_client, query):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ ClickHouse –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç DataFrame."""
    try:
        data, columns = _client.execute(query, with_column_types=True)
        column_names = [col[0] for col in columns]
        df = pd.DataFrame(data, columns=column_names)
        return df
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è SQL-–∑–∞–ø—Ä–æ—Å–∞: {e}")
        st.code(query)
        return pd.DataFrame()


def get_country_iso_alpha3(country_name):
    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Å—Ç—Ä–∞–Ω—ã –≤ ISO Alpha-3 –∫–æ–¥ –¥–ª—è –∫–∞—Ä—Ç—ã."""
    try:
        return pc.country_name_to_country_alpha3(country_name)
    except:
        return None

# --- –û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å ---
st.title("üìä –ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞ –ª–æ–≥–æ–≤ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ (Star Schema)")


# --- –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å —Å —Ñ–∏–ª—å—Ç—Ä—ã ---
st.sidebar.title("–§–∏–ª—å—Ç—Ä—ã")

# --- –ü–†–û–í–ï–†–ö–ê –ì–û–¢–û–í–ù–û–°–¢–ò DIM –¢–ê–ë–õ–ò–¶ (–ù–û–í–û–ï) ---
dim_check_df = run_query(CLIENT, "SELECT count() FROM dim_ip")
if dim_check_df.empty or dim_check_df.iloc[0, 0] == 0:
    st.error("‚ö†Ô∏è DIM —Ç–∞–±–ª–∏—Ü–∞ 'dim_ip' –ø—É—Å—Ç–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ 'spark_processor' –∑–∞–ø—É—â–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–ª –ø–µ—Ä–≤—ã–µ –ª–æ–≥–∏.")
    st.stop() # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–æ–∫ JOIN
# -------------------------------------------------

# –ó–∞–ø—Ä–æ—Å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –≤—Ä–µ–º–µ–Ω–∏ —Ç–µ–ø–µ—Ä—å –∏–¥–µ—Ç –∫ —Ç–∞–±–ª–∏—Ü–µ –§–∞–∫—Ç–æ–≤
min_max_time_df = run_query(CLIENT, "SELECT min(time_key), max(time_key) FROM fact_nginx_requests")
if not min_max_time_df.empty and min_max_time_df.iloc[0, 0] is not None:
    min_ts = min_max_time_df.iloc[0, 0]
    max_ts = min_max_time_df.iloc[0, 1]

    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º pandas.Timestamp –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π python datetime
    min_dt = min_ts.to_pydatetime()
    max_dt = max_ts.to_pydatetime()

    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å–ª–∏ min == max
    if min_dt >= max_dt:
        max_dt = min_dt + timedelta(minutes=1) 
        st.warning("–í –¥–∞–Ω–Ω—ã—Ö –æ–±–Ω–∞—Ä—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω –≤—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª. –°–ª–∞–π–¥–µ—Ä —Ä–∞—Å—à–∏—Ä–µ–Ω –Ω–∞ 1 –º–∏–Ω—É—Ç—É.")

    time_range = st.sidebar.slider(
        "–í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω",
        min_value=min_dt,
        max_value=max_dt,
        value=(min_dt, max_dt),
        format="DD/MM/YYYY - HH:mm",
    )
    start_time, end_time = time_range
else:
    start_time, end_time = datetime.now() - timedelta(hours=1), datetime.now()

# –ó–∞–ø—Ä–æ—Å—ã –∫ DIM —Ç–∞–±–ª–∏—Ü–∞–º –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
statuses_df = run_query(CLIENT, "SELECT DISTINCT status FROM fact_nginx_requests WHERE status IS NOT NULL ORDER BY status")
methods_df = run_query(CLIENT, "SELECT DISTINCT method FROM fact_nginx_requests WHERE method IS NOT NULL AND method != '' ORDER BY method")

# –ó–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω (–∏–∑ DIM —Ç–∞–±–ª–∏—Ü—ã)
countries_df = run_query(CLIENT, "SELECT DISTINCT country FROM dim_ip WHERE country IS NOT NULL AND country != 'Unknown' AND country != 'Error' ORDER BY country")

all_statuses = statuses_df["status"].tolist() if not statuses_df.empty else []
all_countries = countries_df["country"].tolist() if not countries_df.empty else []
all_methods = methods_df["method"].tolist() if not methods_df.empty else []

selected_statuses = st.sidebar.multiselect("–°—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞", all_statuses, default=all_statuses)
selected_countries = st.sidebar.multiselect("–°—Ç—Ä–∞–Ω–∞", all_countries, default=all_countries)
selected_methods = st.sidebar.multiselect("–ú–µ—Ç–æ–¥ –∑–∞–ø—Ä–æ—Å–∞", all_methods, default=all_methods)

if st.sidebar.button("üîÑ –ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –∏ –æ–±–Ω–æ–≤–∏—Ç—å"):
    st.rerun()

# --- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ SQL-—É—Å–ª–æ–≤–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–∏–ª—å—Ç—Ä–æ–≤ ---
where_clauses = [f"T1.time_key BETWEEN toDateTime('{start_time}') AND toDateTime('{end_time}')"]

# –§–∏–ª—å—Ç—Ä—ã –¥–ª—è Fact Table
if selected_statuses and len(selected_statuses) != len(all_statuses):
    where_clauses.append(f"T1.status IN {tuple(selected_statuses)}")
if selected_methods and len(selected_methods) != len(all_methods):
    where_clauses.append(f"T1.method IN {tuple(selected_methods)}")

# –§–∏–ª—å—Ç—Ä –¥–ª—è DIM IP (–°—Ç—Ä–∞–Ω–∞)
if selected_countries and len(selected_countries) != len(all_countries):
    # JOIN –∏ —Ñ–∏–ª—å—Ç—Ä –ø–æ DIM —Ç–∞–±–ª–∏—Ü–µ
    where_clauses.append(f"T2.country IN {tuple(selected_countries)}")

where_sql = " AND ".join(where_clauses)
if where_sql:
    where_sql = "WHERE " + where_sql

# --- KPI-–º–µ—Ç—Ä–∏–∫–∏ ---
kpi_query = f"""
SELECT
    count() as total,
    uniq(T1.ip_key) as unique_ips,
    avg(T1.bytes) as avg_bytes,
    (countIf(T1.status >= 500) / toFloat64(countIf(true))) * 100 as server_error_rate,
    (countIf(T1.status >= 400 AND T1.status < 500) / toFloat64(countIf(true))) * 100 as client_error_rate
FROM fact_nginx_requests AS T1
INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
{where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
"""
kpi_df = run_query(CLIENT, kpi_query)
if not kpi_df.empty:
    kpi_data = kpi_df.iloc[0]
    total_requests, unique_ips, avg_bytes, server_error_rate, client_error_rate = (
        kpi_data.get("total", 0), kpi_data.get("unique_ips", 0), kpi_data.get("avg_bytes", 0),
        kpi_data.get("server_error_rate", 0.0), kpi_data.get("client_error_rate", 0.0)
    )
else:
    total_requests, unique_ips, avg_bytes, server_error_rate, client_error_rate = (0, 0, 0, 0.0, 0.0)

kpi1, kpi2, kpi3, kpi4, kpi5 = st.columns(5)
kpi1.metric("–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤", f"{total_requests:,}")
kpi2.metric("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ IP", f"{unique_ips:,}")
kpi3.metric("–°—Ä–µ–¥–Ω–∏–π –æ—Ç–≤–µ—Ç (–±–∞–π—Ç)", f"{int(avg_bytes):,}")
kpi4.metric("–û—à–∏–±–∫–∏ –∫–ª–∏–µ–Ω—Ç–∞ (4xx %)", f"{client_error_rate:.2f}%")
kpi5.metric("–û—à–∏–±–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞ (5xx %)", f"{server_error_rate:.2f}%")
st.markdown("---")

# --- –í–∫–ª–∞–¥–∫–∏ —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ ---
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(
    ["üìà –û–±–∑–æ—Ä –∏ –¥–∏–Ω–∞–º–∏–∫–∞", "üåç –ì–µ–æ-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞", "üö¶ –¢–æ–ø-–ª–∏—Å—Ç—ã –∏ —Å—Ç–∞—Ç—É—Å—ã", "üö® –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π", "üîß –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ —Å–µ—Ä–≤–µ—Ä–∞", "üîÆ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"]
)

# --- –í–ö–õ–ê–î–ö–ê 1: –û–±–∑–æ—Ä –∏ –¥–∏–Ω–∞–º–∏–∫–∞ ---
with tab1:
    st.subheader("–î–∏–Ω–∞–º–∏–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–∏–ø–∞–º –æ—Ç–≤–µ—Ç–æ–≤ (Stacked Area Chart)")
    time_series_query_stacked = f"""
    SELECT
        toStartOfMinute(T1.time_key) as minute,
        countIf(T1.status >= 200 AND T1.status < 300) as success_2xx,
        countIf(T1.status >= 300 AND T1.status < 400) as redirects_3xx,
        countIf(T1.status >= 400 AND T1.status < 500) as client_errors_4xx,
        countIf(T1.status >= 500) as server_errors_5xx
    FROM fact_nginx_requests AS T1
    {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
    GROUP BY minute ORDER BY minute
    """
    df_time_stacked = run_query(CLIENT, time_series_query_stacked)
    if not df_time_stacked.empty:
        st.area_chart(df_time_stacked.set_index("minute"))

    st.subheader("–î–∏–Ω–∞–º–∏–∫–∞ —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç–≤–µ—Ç–∞ (–≤ –±–∞–π—Ç–∞—Ö)")
    avg_bytes_query = f"""
    SELECT
        toStartOfMinute(T1.time_key) as minute,
        avg(T1.bytes) as avg_bytes
    FROM fact_nginx_requests AS T1
    {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
    GROUP BY minute ORDER BY minute
    """
    df_avg_bytes = run_query(CLIENT, avg_bytes_query)
    if not df_avg_bytes.empty:
        st.line_chart(df_avg_bytes.set_index("minute"))

# --- –í–ö–õ–ê–î–ö–ê 2: –ì–µ–æ-–∞–Ω–∞–ª–∏—Ç–∏–∫–∞ ---
with tab2:
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("–ö–∞—Ä—Ç–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º")
        # JOIN Fact —Å Dim IP
        country_query = f"""
        SELECT 
            T2.country, 
            count() as cnt
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
        {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
        GROUP BY T2.country
        """
        df_country = run_query(CLIENT, country_query)
        if not df_country.empty:
            df_country["iso_alpha"] = df_country["country"].apply(get_country_iso_alpha3)
            df_country = df_country.dropna(subset=["iso_alpha"])
            fig = px.choropleth(df_country, locations="iso_alpha", color="cnt", hover_name="country",
                                color_continuous_scale=px.colors.sequential.Plasma, title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤")
            st.plotly_chart(fig, use_container_width=True)

    with col2:
        st.subheader("–ö–∞—Ä—Ç–∞ —É—Ä–æ–≤–Ω—è –æ—à–∏–±–æ–∫ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º")
        # JOIN Fact —Å Dim IP –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—à–∏–±–æ–∫
        country_error_query = f"""
        SELECT
            T2.country,
            countIf(T1.status >= 400) as error_count,
            count() as total_count,
            (error_count / toFloat64(total_count)) * 100 as error_rate
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
        {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
        GROUP BY T2.country HAVING total_count > 0
        """
        df_country_errors = run_query(CLIENT, country_error_query)
        if not df_country_errors.empty:
            df_country_errors["iso_alpha"] = df_country_errors["country"].apply(get_country_iso_alpha3)
            df_country_errors = df_country_errors.dropna(subset=["iso_alpha"])
            fig_errors = px.choropleth(df_country_errors, locations="iso_alpha", color="error_rate", hover_name="country",
                                       color_continuous_scale=px.colors.sequential.Reds, title="–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ (%)")
            st.plotly_chart(fig_errors, use_container_width=True)

    st.subheader("–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —Å—Ç—Ä–∞–Ω–∞–º –∏ –æ—à–∏–±–∫–∞–º")
    if not df_country_errors.empty:
        st.dataframe(df_country_errors[['country', 'total_count', 'error_count', 'error_rate']].sort_values('error_rate', ascending=False), use_container_width=True)


# --- –í–ö–õ–ê–î–ö–ê 3: –¢–æ–ø-–ª–∏—Å—Ç—ã –∏ —Å—Ç–∞—Ç—É—Å—ã ---
with tab3:
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("–¢–æ–ø 10 —Å—Ç—Ä–∞–Ω–∏—Ü –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º")
        # JOIN Fact —Å Dim IP –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞–Ω–µ, –µ—Å–ª–∏ –æ–Ω–∞ –≤—ã–±—Ä–∞–Ω–∞
        pages_query = f"""
        SELECT page, count() AS hits 
        FROM fact_nginx_requests AS T1
        {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
        GROUP BY page ORDER BY hits DESC LIMIT 10
        """
        pages_df = run_query(CLIENT, pages_query)
        st.dataframe(pages_df, use_container_width=True)

        st.subheader("–¢–æ–ø 10 IP –ø–æ –æ–±—ä–µ–º—É —Ç—Ä–∞—Ñ–∏–∫–∞ (MB)")
        # JOIN Fact —Å Dim IP –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è IP –∞–¥—Ä–µ—Å–∞
        ip_traffic_query = f"""
        SELECT 
            T2.ip, 
            sum(T1.bytes) / 1024 / 1024 as total_mb 
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
        {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND ")}
        GROUP BY T2.ip ORDER BY total_mb DESC LIMIT 10
        """
        ip_traffic_df = run_query(CLIENT, ip_traffic_query)
        if not ip_traffic_df.empty:
            st.bar_chart(ip_traffic_df.set_index('ip'))

    with col2:
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º")
        status_query = f"SELECT status, count() AS cnt FROM fact_nginx_requests {where_sql} AND log_type = 'access' GROUP BY status ORDER BY status"
        status_df = run_query(CLIENT, status_query)
        if not status_df.empty:
            fig = px.pie(status_df, names="status", values="cnt", title="–°—Ç–∞—Ç—É—Å—ã –æ—Ç–≤–µ—Ç–æ–≤")
            st.plotly_chart(fig, use_container_width=True)

        st.subheader("–¢–æ–ø 10 IP –ø–æ –æ—à–∏–±–∫–∞–º")
        # JOIN Fact —Å Dim IP
        ip_errors_query = f"""
        SELECT 
            T2.ip, 
            count() as errors 
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
        {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND T1.status >= 400 AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND T1.status >= 400 AND ")}
        GROUP BY T2.ip ORDER BY errors DESC LIMIT 10
        """
        ip_errors_df = run_query(CLIENT, ip_errors_query)
        st.dataframe(ip_errors_df, use_container_width=True)

    st.subheader("–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –æ—à–∏–±–æ–∫: –°—Ç—Ä–∞–Ω–∏—Ü–∞ vs –°—Ç–∞—Ç—É—Å")
    # JOIN Fact —Å Dim IP (–¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä–∞–Ω–µ, –µ—Å–ª–∏ –≤—ã–±—Ä–∞–Ω–∞) –∏ DIM Anomaly
    heatmap_query = f"""
    SELECT T1.page, T1.status, count() as count
    FROM fact_nginx_requests AS T1
    -- JOIN —Å Dim IP, —á—Ç–æ–±—ã –ø—Ä–∏–º–µ–Ω–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä—ã –ø–æ —Å—Ç—Ä–∞–Ω–µ
    INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id 
    {where_sql.replace("WHERE", "WHERE T1.log_type = 'access' AND T1.status >= 400 AND " if "WHERE" in where_sql else "WHERE T1.log_type = 'access' AND T1.status >= 400 AND ")}
    AND T1.page IN (
        SELECT page FROM fact_nginx_requests AS T_inner
        {where_sql.replace("WHERE", "WHERE T_inner.log_type = 'access' AND " if "WHERE" in where_sql else "WHERE T_inner.log_type = 'access' AND ")}
        GROUP BY page ORDER BY count() DESC LIMIT 15
    )
    GROUP BY T1.page, T1.status
    """
    heatmap_df = run_query(CLIENT, heatmap_query)
    if not heatmap_df.empty:
        heatmap_pivot = heatmap_df.pivot_table(index='page', columns='status', values='count').fillna(0)
        fig_heatmap = px.imshow(heatmap_pivot, text_auto=True, aspect="auto",
                                color_continuous_scale='Reds',
                                labels=dict(x="HTTP –°—Ç–∞—Ç—É—Å", y="–°—Ç—Ä–∞–Ω–∏—Ü–∞", color="–ö–æ–ª-–≤–æ –æ—à–∏–±–æ–∫"))
        st.plotly_chart(fig_heatmap, use_container_width=True)

# --- –í–ö–õ–ê–î–ö–ê 4: –î–µ—Ç–µ–∫—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π ---
with tab4:
    st.subheader("–û–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ –∞–Ω–æ–º–∞–ª–∏–∏")
    # –§–∏–ª—å—Ç—Ä –ø–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–º—É –¥–∏–∞–ø–∞–∑–æ–Ω—É –∏ –∞–Ω–æ–º–∞–ª–∏—è–º
    anomaly_where = f"WHERE T1.time_key BETWEEN toDateTime('{start_time}') AND toDateTime('{end_time}')"

    col1, col2 = st.columns([2,1])
    with col1:
        st.subheader("–í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ –∞–Ω–æ–º–∞–ª–∏–π (Timeline)")
        # JOIN Fact —Å Dim Anomaly –∏ Dim IP
        anomaly_timeline_query = f"""
        SELECT 
            T1.time_key as timestamp, 
            T3.ip, 
            T2.anomaly_type
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_anomaly_type AS T2 ON T1.anomaly_type_key = T2.anomaly_type_id
        INNER JOIN dim_ip AS T3 ON T1.ip_key = T3.ip_id
        {anomaly_where} AND T1.is_anomaly = 1 AND T2.anomaly_type != 'NoAnomaly' 
        ORDER BY timestamp DESC LIMIT 500
        """
        df_anomalies_timeline = run_query(CLIENT, anomaly_timeline_query)
        if not df_anomalies_timeline.empty:
            fig_timeline = px.scatter(df_anomalies_timeline, x='timestamp', y='ip', color='anomaly_type',
                                      title="–í—Ä–µ–º–µ–Ω–Ω–∞—è —à–∫–∞–ª–∞ –∞–Ω–æ–º–∞–ª—å–Ω–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏",
                                      labels={"timestamp": "–í—Ä–µ–º—è", "ip": "IP –∞–¥—Ä–µ—Å –∞—Ç–∞–∫—É—é—â–µ–≥–æ", "anomaly_type": "–¢–∏–ø –∞–Ω–æ–º–∞–ª–∏–∏"})
            st.plotly_chart(fig_timeline, use_container_width=True)
        else:
            st.info("–ê–Ω–æ–º–∞–ª—å–Ω–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.")

    with col2:
        st.subheader("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–æ–º–∞–ª–∏–π")
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ Dim Anomaly
        anomaly_pie_query = f"""
        SELECT 
            T2.anomaly_type, 
            count() as cnt 
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_anomaly_type AS T2 ON T1.anomaly_type_key = T2.anomaly_type_id
        {anomaly_where} AND T1.is_anomaly = 1 AND T2.anomaly_type != 'NoAnomaly' 
        GROUP BY T2.anomaly_type
        """
        df_anomaly_pie = run_query(CLIENT, anomaly_pie_query)
        if not df_anomaly_pie.empty:
            fig_pie = px.pie(df_anomaly_pie, names='anomaly_type', values='cnt')
            st.plotly_chart(fig_pie, use_container_width=True)

    st.subheader("–°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –ø–æ –∞–Ω–æ–º–∞–ª–∏—è–º")
    # JOIN Fact —Å Dim IP –∏ Dim Anomaly
    anomaly_table_query = f"""
    SELECT 
        T3.ip, 
        T3.country, 
        T2.anomaly_type, 
        max(T1.time_key) as last_seen, 
        count() as request_count 
    FROM fact_nginx_requests AS T1
    INNER JOIN dim_anomaly_type AS T2 ON T1.anomaly_type_key = T2.anomaly_type_id
    INNER JOIN dim_ip AS T3 ON T1.ip_key = T3.ip_id
    {anomaly_where} AND T1.is_anomaly = 1 AND T2.anomaly_type != 'NoAnomaly' 
    GROUP BY T3.ip, T3.country, T2.anomaly_type 
    ORDER BY last_seen DESC LIMIT 20
    """
    df_anomalies_table = run_query(CLIENT, anomaly_table_query)
    if not df_anomalies_table.empty:
        st.dataframe(df_anomalies_table, use_container_width=True)

# --- –í–ö–õ–ê–î–ö–ê 5: –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ —Å–µ—Ä–≤–µ—Ä–∞ ---
with tab5:
    st.subheader("–ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤ –æ—à–∏–±–æ–∫")
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –ª–æ–≥–∞–º –æ—à–∏–±–æ–∫
    error_where = f"WHERE log_type = 'error' AND time_key BETWEEN toDateTime('{start_time}') AND toDateTime('{end_time}')"
    
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("–¢–æ–ø 10 —Å–æ–æ–±—â–µ–Ω–∏–π –æ–± –æ—à–∏–±–∫–∞—Ö")
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ –ø–æ–ª—é error_message –≤ Fact Table
        top_errors_query = f"SELECT error_message, count() as cnt FROM fact_nginx_requests {error_where} GROUP BY error_message ORDER BY cnt DESC LIMIT 10"
        df_top_errors = run_query(CLIENT, top_errors_query)
        if not df_top_errors.empty:
            fig_top_errors = px.bar(df_top_errors, x='cnt', y='error_message', orientation='h', title="–°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –æ—à–∏–±–∫–∏")
            st.plotly_chart(fig_top_errors, use_container_width=True)

    with col2:
        st.subheader("–î–∏–Ω–∞–º–∏–∫–∞ –æ—à–∏–±–æ–∫ –ø–æ —É—Ä–æ–≤–Ω—è–º (error/warn) - **–¢—Ä–µ–±—É–µ—Ç DIM_ANOMALY**")
        # –î–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º error_message –∫–∞–∫ –ø—Ä–æ–∫—Å–∏
        error_level_query = f"""
        SELECT
            toStartOfMinute(time_key) as minute,
            countIf(error_message LIKE '%error%') as errors, 
            countIf(error_message LIKE '%warn%') as warnings
        FROM fact_nginx_requests {error_where}
        GROUP BY minute ORDER BY minute
        """
        df_error_level = run_query(CLIENT, error_level_query)
        if not df_error_level.empty and (df_error_level['errors'].sum() > 0 or df_error_level['warnings'].sum() > 0):
            st.line_chart(df_error_level.set_index('minute'))

    st.subheader("–ü–æ—Å–ª–µ–¥–Ω–∏–µ 100 –æ—à–∏–±–æ–∫ —Å–µ—Ä–≤–µ—Ä–∞")
    # JOIN Fact —Å Dim IP
    df_errors_table = run_query(CLIENT, f"""
        SELECT 
            T1.time_key as timestamp, 
            T2.ip, 
            T2.country, 
            T1.log_level, -- –≠—Ç–æ –ø–æ–ª–µ Nullable –≤ Fact Table
            T1.error_message 
        FROM fact_nginx_requests AS T1
        INNER JOIN dim_ip AS T2 ON T1.ip_key = T2.ip_id
        {error_where} 
        ORDER BY timestamp DESC LIMIT 100
    """)
    if not df_errors_table.empty:
        st.dataframe(df_errors_table, use_container_width=True)
    else:
        st.info("–û—à–∏–±–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ –≤—ã–±—Ä–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ.")
        
with tab6:
    st.subheader("–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞–≥—Ä—É–∑–∫–∏ –Ω–∞ —Å–µ—Ä–≤–µ—Ä (–∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —á–∞—Å)")
    
    # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 3 –¥–Ω—è
    actuals_query = """
    SELECT 
        toStartOfHour(time_key) as hour, 
        count() as actual_requests
    FROM fact_nginx_requests
    WHERE log_type = 'access' AND time_key >= now() - INTERVAL 3 DAY
    GROUP BY hour ORDER BY hour
    """
    df_actuals = run_query(CLIENT, actuals_query)
    
    # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (—Ç–∞–±–ª–∏—Ü–∞ –æ—Å—Ç–∞–ª–∞—Å—å –ø—Ä–µ–∂–Ω–µ–π)
    predictions_query = "SELECT timestamp as hour, predicted_requests, predicted_lower, predicted_upper FROM nginx_predictions ORDER BY hour"
    df_predictions = run_query(CLIENT, predictions_query)

    if not df_actuals.empty and not df_predictions.empty:
        # --- –ë–ª–æ–∫ –ø—Ä–µ–¥–ø–∏—Å—ã–≤–∞—é—â–µ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ ---
        CRITICAL_LOAD_THRESHOLD = df_actuals['actual_requests'].quantile(0.95) # –ü–æ—Ä–æ–≥ = 95-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
        
        future_predictions = df_predictions[df_predictions['hour'] > datetime.now()]
        
        if not future_predictions.empty:
            peak_prediction = future_predictions.sort_values('predicted_upper', ascending=False).iloc[0]

            st.info(f"**–ü—Ä–æ–≥–Ω–æ–∑:** –û–∂–∏–¥–∞–µ—Ç—Å—è –ø–∏–∫–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ **~{int(peak_prediction['predicted_requests'])}** –∑–∞–ø—Ä–æ—Å–æ–≤/—á–∞—Å –≤ **{peak_prediction['hour'].strftime('%Y-%m-%d %H:%M')}**.")

            if peak_prediction['predicted_upper'] > CRITICAL_LOAD_THRESHOLD:
                st.error(
                    f"""
                    **‚ö†Ô∏è –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø (–ü—Ä–µ–¥–ø–∏—Å—ã–≤–∞—é—â–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞):**
                    –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è –ø–∏–∫–æ–≤–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ ({int(peak_prediction['predicted_upper'])} –∑–∞–ø—Ä–æ—Å–æ–≤/—á–∞—Å) –ø—Ä–µ–≤—ã—à–∞–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä–æ–≥ ({int(CRITICAL_LOAD_THRESHOLD)} –∑–∞–ø—Ä–æ—Å–æ–≤/—á–∞—Å).
                    **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤ –≤–µ–±-—Å–µ—Ä–≤–µ—Ä–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —É–≤–µ–ª–∏—á–µ–Ω–∏—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–æ–¥–æ–≤/–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤) –ø–µ—Ä–µ–¥ –ø–∏–∫–æ–≤—ã–º –≤—Ä–µ–º–µ–Ω–µ–º.**
                    """
                )
            else:
                st.success(
                    """
                    **‚úÖ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø (–ü—Ä–µ–¥–ø–∏—Å—ã–≤–∞—é—â–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞):**
                    –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –Ω–æ—Ä–º—ã. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.
                    """
                )

            # --- –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è ---
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–ª—è Altair
            df_actuals['type'] = '–§–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ'
            df_actuals.rename(columns={'actual_requests': 'requests'}, inplace=True)
            
            df_pred_main = df_predictions[['hour', 'predicted_requests']].copy()
            df_pred_main['type'] = '–ü—Ä–æ–≥–Ω–æ–∑'
            df_pred_main.rename(columns={'predicted_requests': 'requests'}, inplace=True)

            # –°–æ–µ–¥–∏–Ω—è–µ–º –¥–ª—è –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞
            source = pd.concat([df_actuals[['hour', 'requests', 'type']], df_pred_main])

            # –û—Å–Ω–æ–≤–Ω–æ–π –≥—Ä–∞—Ñ–∏–∫
            line = alt.Chart(source).mark_line().encode(
                x='hour:T',
                y='requests:Q',
                color='type:N'
            ).properties(
                 title='–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∞'
            )

            # –û–±–ª–∞—Å—Ç—å –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞
            band = alt.Chart(df_predictions).mark_area(opacity=0.3).encode(
                x='hour:T',
                y='predicted_lower:Q',
                y2='predicted_upper:Q'
            ).properties(
                title='–î–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≥–Ω–æ–∑–∞'
            )
            
            st.altair_chart((band + line).interactive(), use_container_width=True)
        else:
            st.warning("–ù–µ—Ç –±—É–¥—É—â–∏—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è.")
    else:
        st.warning("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞. –°–Ω–∞—á–∞–ª–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–∫—Ä–∏–ø—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤.")
```

### streamlit/Dockerfile

```
    
FROM python:3.11-slim

RUN pip install --no-cache-dir streamlit pandas clickhouse-driver plotly pycountry-convert

WORKDIR /app
COPY dashboard.py /app/dashboard.py

EXPOSE 8501
CMD ["streamlit", "run", "dashboard.py", "--server.port=8501", "--server.address=0.0.0.0"]
```

### spark/make_predictions.py

```
import pandas as pd
from clickhouse_driver import Client
import pickle
import os # <-- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£

CLICKHOUSE_HOST = 'clickhouse'
MODEL_DIR = '/opt/spark-apps/model' # <-- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£
MODEL_PATH = os.path.join(MODEL_DIR, 'prophet_model.pkl') # <-- –ò–ó–ú–ï–ù–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£
PREDICTIONS_TABLE = 'nginx_predictions'

print("--- –ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ ---")

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
print(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞: {MODEL_PATH}")
with open(MODEL_PATH, 'rb') as f:
    model = pickle.load(f)
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

# 2. –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–∞ –¥–ª—è –±—É–¥—É—â–µ–≥–æ
future = model.make_future_dataframe(periods=24, freq='H') # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 24 —á–∞—Å–∞ –≤–ø–µ—Ä–µ–¥
forecast = model.predict(future)
print("‚úÖ –ü—Ä–æ–≥–Ω–æ–∑ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω.")

# 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è ClickHouse
forecast_to_save = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].copy()
forecast_to_save.rename(columns={
    'ds': 'timestamp',
    'yhat': 'predicted_requests',
    'yhat_lower': 'predicted_lower',
    'yhat_upper': 'predicted_upper'
}, inplace=True)

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–¥—É—â–∏–µ –ø—Ä–æ–≥–Ω–æ–∑—ã
now = pd.Timestamp.now().tz_localize(None)
forecast_to_save = forecast_to_save[forecast_to_save['timestamp'] > now]

# 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ ClickHouse
client = Client(host=CLICKHOUSE_HOST)
print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ —Ç–∞–±–ª–∏—Ü—É {PREDICTIONS_TABLE}...")

# –û—á–∏—â–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –∏ –≤—Å—Ç–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ
client.execute(f'TRUNCATE TABLE {PREDICTIONS_TABLE}')
print("–°—Ç–∞—Ä—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã —É–¥–∞–ª–µ–Ω—ã.")

client.execute(
    f'INSERT INTO {PREDICTIONS_TABLE} VALUES',
    forecast_to_save.to_dict('records')
)
print(f"‚úÖ {len(forecast_to_save)} —Å—Ç—Ä–æ–∫ –ø—Ä–æ–≥–Ω–æ–∑–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ ClickHouse.")

print("--- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ---")
```

### spark/spark_processor.py

```
# spark/spark_processor.py
import time
import geoip2.database
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError
from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    udf,
    col,
    from_json,
    when,
    lit,
    count,
    countDistinct,
    regexp_extract,
    to_timestamp,
    split,
    greatest,
    coalesce,
    hash, 
    date_trunc 
)
from pyspark.sql.types import StringType, StructField, StructType, IntegerType

KAFKA_BROKER = "kafka:9092"
TOPIC = "nginx_logs"
CHECKPOINT_DIR = "/tmp/spark_checkpoints_nginx_v3" # –ò–ó–ú–ï–ù–ï–ù–û: –ù–æ–≤—ã–π –ø—É—Ç—å –¥–ª—è —Å–±—Ä–æ—Å–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è
INIT_SCRIPT_EXECUTED = False # –§–ª–∞–≥ –¥–ª—è –æ–¥–Ω–æ–∫—Ä–∞—Ç–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ DIM

# –ü–æ–≤–µ–¥–µ–Ω—á–µ—Å–∫–∏–µ –ø–æ—Ä–æ–≥–∏
REQUEST_RATE_THRESHOLD = 20
LOGIN_ATTACK_THRESHOLD = 10
SCANNING_THRESHOLD = 15

# –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å–∏–≥–Ω–∞—Ç—É—Ä–Ω—ã—Ö –∞—Ç–∞–∫
SQLI_PATTERN = r"('|%27|--|%2D%2D|union|%75%6E%69%6F%6E)"
PATH_TRAVERSAL_PATTERN = r"(\.\./|%2E%2E%2F)"
VULN_SCAN_PATTERN = r"(wp-admin|phpmyadmin|/.git|/solr)"
BAD_AGENT_PATTERN = r"(sqlmap|nikto|nmap|masscan)"

CLICKHOUSE_URL = "jdbc:clickhouse://clickhouse:8123/default"
CLICKHOUSE_FACT_TABLE = "fact_nginx_requests"
CLICKHOUSE_DIM_IP = "dim_ip"
CLICKHOUSE_DIM_ANOMALY = "dim_anomaly_type"
GEO_DB_PATH = "/opt/spark-apps/GeoLite2-Country.mmdb"

def ensure_topic():
    for i in range(10):
        try:
            admin = KafkaAdminClient(bootstrap_servers=KAFKA_BROKER, client_id="spark-topic-checker")
            topic_list = [NewTopic(name=TOPIC, num_partitions=1, replication_factor=1)]
            admin.create_topics(new_topics=topic_list, validate_only=False)
            print(f"‚úÖ Kafka topic '{TOPIC}' —Å–æ–∑–¥–∞–Ω.")
            admin.close()
            return
        except TopicAlreadyExistsError:
            print(f"‚ÑπÔ∏è Kafka topic '{TOPIC}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç.")
            admin.close()
            return
        except Exception as e:
            print(f"‚ö†Ô∏è Kafka –ø–æ–∫–∞ –Ω–µ –≥–æ—Ç–æ–≤ ({e}), –∂–¥—ë–º...")
            time.sleep(5)
    print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å Kafka-—Ç–æ–ø–∏–∫. –ü—Ä–æ–≤–µ—Ä—å kafka logs.")

ensure_topic()

spark = SparkSession.builder.appName("NginxLogProcessorStarSchema").config("spark.sql.streaming.checkpointLocation", CHECKPOINT_DIR).getOrCreate()

@udf(StringType())
def get_country_from_ip(ip):
    try:
        with geoip2.database.Reader(GEO_DB_PATH) as reader:
            response = reader.country(ip)
            return response.country.name
    except (geoip2.errors.AddressNotFoundError, ValueError):
        return "Unknown"
    except Exception:
        return "Error"

kafka_schema = StructType([StructField("message", StringType()), StructField("log_type", StringType())])
df = spark.readStream.format("kafka").option("kafka.bootstrap.servers", KAFKA_BROKER).option("subscribe", TOPIC).option("startingOffsets", "earliest").load()
json_df = df.select(from_json(col("value").cast("string"), kafka_schema).alias("data")).select("data.*")

access_pattern = r'(\S+) - - \[(.*?)\] "(\S+)\s*(\S*)\s*(\S*)" (\d{3}) (\d+) "(.*?)" "(.*?)"'
error_pattern = r'(\d{4}/\d{2}/\d{2} \d{2}:\d{2}:\d{2}) \[(\w+)\] .*? client: (\S+), server: .*?, request: ".*?", (.*?), host: ".*?"'

access_logs = json_df.filter(col("log_type") == "access").select(
    regexp_extract("message", access_pattern, 1).alias("ip"),
    regexp_extract("message", access_pattern, 2).alias("time"),
    regexp_extract("message", access_pattern, 3).alias("method"),
    regexp_extract("message", access_pattern, 4).alias("page"),
    regexp_extract("message", access_pattern, 6).alias("status"),
    regexp_extract("message", access_pattern, 7).alias("bytes"),
    regexp_extract("message", access_pattern, 8).alias("referrer"),
    regexp_extract("message", access_pattern, 9).alias("agent"),
    lit("access").alias("log_type"),
).withColumn("request", col("page")).withColumn("timestamp", to_timestamp(col("time"), "dd/MMM/yyyy:HH:mm:ss Z")).withColumn("status", col("status").cast(IntegerType())).withColumn("bytes", col("bytes").cast(IntegerType())).withColumn("error_message", lit(None).cast(StringType())).withColumn("log_level", lit(None).cast(StringType())).drop("time")

# –ò–°–ü–†–ê–í–õ–ï–ù–û: –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –∏ –±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è IP –≤ –ª–æ–≥–∞—Ö –æ—à–∏–±–æ–∫
# –í spark_processor.py, –∑–∞–º–µ–Ω–∏—Ç–µ —Å—Ç–∞—Ä—ã–π –±–ª–æ–∫ error_logs:
error_logs = (
    json_df.filter(col("log_type") == "error")
    .select(
        regexp_extract("message", error_pattern, 1).alias("time"),
        regexp_extract("message", error_pattern, 2).alias("log_level"),
        regexp_extract("message", error_pattern, 3).alias("ip_raw"),
        regexp_extract("message", error_pattern, 4).alias("error_message"),
        lit("error").alias("log_type"),
    )
    .withColumn("ip", regexp_extract(col("ip_raw"), r'^(\S+)', 1))
    .withColumn("ip", when(col("ip") != "", col("ip")).otherwise(lit("0.0.0.0")))
    .withColumn("timestamp", to_timestamp(col("time"), "yyyy/MM/dd HH:mm:ss"))
    .select(
        "timestamp",
        "ip",
        "log_type",
        "log_level",
        "error_message",
        lit(None).cast(StringType()).alias("request"),
        lit(None).cast(StringType()).alias("method"),
        lit(None).cast(StringType()).alias("page"),
        lit(0).cast(IntegerType()).alias("status"),
        lit(0).cast(IntegerType()).alias("bytes"),
        lit(None).cast(StringType()).alias("referrer"),
        lit(None).cast(StringType()).alias("agent"),
    )
)


unified_df = access_logs.unionByName(error_logs).filter(col("ip") != "")


def write_to_clickhouse(batch_df, batch_id):
    if batch_df.rdd.isEmpty():
        print(f"‚ö†Ô∏è –ü—É—Å—Ç–æ–π batch {batch_id}, –ø—Ä–æ–ø—É—â–µ–Ω.")
        return

    print(f"--- Processing Batch {batch_id} (Star Schema ETL) ---")
    batch_df.cache()

    # --- 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –î–∞–Ω–Ω—ã—Ö –¥–ª—è DIMENSION (IP/Geo) ---
    dim_ip_df = (
        batch_df.filter(col("ip").isNotNull())
        .select(
            col("ip"), 
            get_country_from_ip(col("ip")).alias("country"),
            hash(col("ip")).alias("ip_id")
        )
        .distinct()
    )
    
    # –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω—ã
    dim_ip_df = dim_ip_df.na.fill({'country': 'Unknown'})
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï (–§–ò–ù–ê–õ–¨–ù–û–ï): –Ø–≤–Ω–æ –ø–µ—Ä–µ–≤—ã–±–∏—Ä–∞–µ–º –í–°–ï 3 —Å—Ç–æ–ª–±—Ü–∞ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –ø–µ—Ä–µ–¥ –∑–∞–ø–∏—Å—å—é. 
    # –≠—Ç–æ –¥–æ–ª–∂–Ω–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å –ø–æ—Ç–µ—Ä—é ip_id –ø–æ—Å–ª–µ .na.fill()
    dim_ip_df = dim_ip_df.select("ip", "country", "ip_id") 
    
    # –í—Å—Ç–∞–≤–ª—è–µ–º/–û–±–Ω–æ–≤–ª—è–µ–º –≤ dim_ip (–∏—Å–ø–æ–ª—å–∑—É—è ReplacingMergeTree)
    (
        dim_ip_df.write.format("jdbc")
        .option("url", CLICKHOUSE_URL)
        .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
        .option("dbtable", CLICKHOUSE_DIM_IP)
        .option("user", "default").option("password", "")
        .mode("append")
        .save()
    )

    # --- 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –î–∞–Ω–Ω—ã—Ö –¥–ª—è DIMENSION (Anomaly Type) ---
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π (—Ç–æ–ª—å–∫–æ —Å–∏–≥–Ω–∞—Ç—É—Ä–Ω—ã–µ –¥–ª—è –∞—Ç–æ–º–∞—Ä–Ω–æ—Å—Ç–∏ —Ñ–∞–∫—Ç–∞)
    enriched_for_dim = batch_df.withColumn("signature_anomaly_type",
        when(col("page").rlike(SQLI_PATTERN), "SQL Injection")
        .when(col("page").rlike(PATH_TRAVERSAL_PATTERN), "Path Traversal")
        .when(col("page").rlike(VULN_SCAN_PATTERN), "Vulnerability Scan")
        .when(col("agent").rlike(BAD_AGENT_PATTERN), "Bad User-Agent")
        .otherwise(lit(None))
    )
    
    dim_anomaly_df = (
        enriched_for_dim.filter(col("signature_anomaly_type").isNotNull())
        .select(col("signature_anomaly_type").alias("anomaly_type"), lit(1).cast(IntegerType()).alias("is_anomaly"))
        .distinct()
    )
    # –í—Å—Ç–∞–≤–ª—è–µ–º/–û–±–Ω–æ–≤–ª—è–µ–º –≤ dim_anomaly_type
    (
        dim_anomaly_df.write.format("jdbc")
        .option("url", CLICKHOUSE_URL)
        .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
        .option("dbtable", CLICKHOUSE_DIM_ANOMALY)
        .option("user", "default").option("password", "")
        .mode("append")
        .save()
    )

    # --- 3. –û–±–æ–≥–∞—â–µ–Ω–∏–µ –∏ –≤—Å—Ç–∞–≤–∫–∞ –≤ FACT Table ---
    
    final_fact_df = (
        enriched_for_dim
        .withColumn("anomaly_type", coalesce(col("signature_anomaly_type"), lit("NoAnomaly")))
        .withColumn("is_anomaly", when(col("anomaly_type") != "NoAnomaly", 1).otherwise(0))
        .withColumn("country", get_country_from_ip(col("ip"))) # –°—Ç—Ä–∞–Ω–∞ –Ω—É–∂–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è DIM, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        .withColumn("time_key", date_trunc("hour", col("timestamp"))) # –ö–ª—é—á –≤—Ä–µ–º–µ–Ω–∏
        .withColumn("ip_key", hash(col("ip"))) # –ö–ª—é—á IP (—Ö—ç—à –æ—Ç IP)
        .withColumn("anomaly_type_key", hash(col("anomaly_type"), col("is_anomaly")) % 255) # –í—ã—á–∏—Å–ª—è–µ–º —Ö—ç—à –Ω–∞–ø—Ä—è–º—É—é
        .withColumn("log_type", coalesce(col("log_type"), lit("unknown")))
    )
    
    (
        final_fact_df.select(
            col("time_key"),
            col("ip_key"),
            col("anomaly_type_key"),
            "log_type",
            col("status"),
            col("bytes"),
            col("error_message"),
            col("method"),
            col("page")
        )
        .write.format("jdbc")
        .option("url", CLICKHOUSE_URL)
        .option("driver", "com.clickhouse.jdbc.ClickHouseDriver")
        .option("dbtable", CLICKHOUSE_FACT_TABLE)
        .option("user", "default")
        .option("password", "")
        .mode("append")
        .save()
    )

    print(f"‚úÖ Batch {batch_id} –∑–∞–ø–∏—Å–∞–Ω –≤ ClickHouse Fact Table. {final_fact_df.count()} —Å—Ç—Ä–æ–∫.")
    batch_df.unpersist()
    
query = unified_df.writeStream.foreachBatch(write_to_clickhouse).outputMode("append").option("checkpointLocation", CHECKPOINT_DIR).trigger(processingTime="15 seconds").start()
query.awaitTermination()
```

### spark/Dockerfile

```
FROM apache/spark:3.4.1

USER root

RUN apt-get update && apt-get install -y python3-pip build-essential libssl-dev libffi-dev python3-dev && \
    pip install --no-cache-dir kafka-python clickhouse-driver pyspark geoip2 prophet

WORKDIR /opt/spark-apps

COPY . /opt/spark-apps

CMD ["/opt/spark/bin/spark-submit", \
     "--packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,com.clickhouse:clickhouse-jdbc:0.4.6", \
     "/opt/spark-apps/spark_processor.py"]
```

### spark/train_model.py

```
import pandas as pd
from prophet import Prophet
from clickhouse_driver import Client
import pickle
import os  # <-- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£

CLICKHOUSE_HOST = 'clickhouse'
MODEL_DIR = '/opt/spark-apps/model' # <-- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£
MODEL_PATH = os.path.join(MODEL_DIR, 'prophet_model.pkl') # <-- –ò–ó–ú–ï–ù–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£

print("--- –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è ---")

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ ClickHouse
print(f"–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ ClickHouse ({CLICKHOUSE_HOST})...")
client = Client(host=CLICKHOUSE_HOST)
query = """
SELECT 
    toStartOfHour(timestamp) as ds,
    count() as y
FROM nginx_logs
WHERE log_type = 'access'
GROUP BY ds
ORDER BY ds
"""
print("–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö...")
data, columns = client.execute(query, with_column_types=True)
df = pd.DataFrame(data, columns=[c[0] for c in columns])
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö.")

df['ds'] = pd.to_datetime(df['ds'])

if len(df) < 2:
    print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –¢—Ä–µ–±—É–µ—Ç—Å—è –∫–∞–∫ –º–∏–Ω–∏–º—É–º 2 —Ç–æ—á–∫–∏.")
    exit()

# 2. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Prophet
print("–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ Prophet...")
model = Prophet(daily_seasonality=True, weekly_seasonality=True)
model.fit(df)
print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞.")

# 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª
print(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –≤ —Ñ–∞–π–ª: {MODEL_PATH}")

# --- –í–û–¢ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï ---
# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é, –µ—Å–ª–∏ –æ–Ω–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
os.makedirs(MODEL_DIR, exist_ok=True) # <-- –î–û–ë–ê–í–ò–¢–¨ –≠–¢–£ –°–¢–†–û–ö–£
# -------------------------

with open(MODEL_PATH, 'wb') as f:
    pickle.dump(model, f)

print("--- –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ ---")
```

